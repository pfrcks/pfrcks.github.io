<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Distance Metrics and Machine Learning | lost in compilation</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/blog/">Blog</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/about/">About</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Distance Metrics and Machine Learning</span></h1>

<h2 class="date">2018/11/28</h2>
</div>

<main>


<h2 id="getting-started">Getting Started</h2>

<p>A distance function(also referred as a metric) in mathematics refers to the function that defines a distance between any two points of a set. The most prevalent example is the Euclidean distance. But let&rsquo;s first define what makes a function a distance function.</p>

<p>For a function to be a distance function, it has to satisfy the following properties:</p>

<ul>
<li>Non-negativity : <code>$d(x, y) \geq 0$</code></li>
<li>Identity       : <code>$d(x, y) = 0 \iff x=y$</code></li>
<li>Symmetry       : <code>$d(x, y) = d(y, x)$</code></li>
<li>Triangle Inequality : <code>$d(x, z) \leq d(x, y) + d(y, z)$</code></li>
</ul>

<p>Some of the distance functions which we went over in class were <code>$L_p$</code> distances, Jaccard Distance, Cosine Distance and Edit distance. {TODO: refer class notes}. Now let&rsquo;s take a look at another kind of <em>distance</em></p>

<h2 id="kullback-leibler-divergence">Kullbackâ€“Leibler divergence</h2>

<p>To call KL divergence a distance is like calling tomato a vegetable. KL divergence is used to measure the difference between two probability distributions over the same random variable x.</p>

<p><code>$$D_{KL}(p(x)||q(x)) = \sum_{x \in X} p(x) \ln \frac{p(x)}{q(x)}$$</code></p>

<p>In terms of information theory, KL divergence measures the expected number of extra bits required to code samples from distribution <code>$p(x)</code> when using code based on <code>$q(x)$</code>. Another way to think about this is that <code>$p(x)</code> is the <strong>true</strong> distribution of data and <code>$q(x)$</code> is our <strong>approximation</strong> of it.</p>

<p>Even though it looks like KL divergence measures <strong>distance</strong> between two probability distributions, it is not a distance measure. KL divergence is neither symmetric not does it satisfy the triangle inequality. It is however non-negative.</p>

<p>If it is not a distance metric, then how is it used? In the formula presented above, the true distribution is often intractable. Therefore by minimizing the KL divergence by using tractable approximated distributions we can get an approximate distribution from which samples can be drawn.</p>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  <hr/>
  &copy; <a href="https://linkedin.com/in/aagrawal92">Amol Agrawal</a> 2017 &ndash; 2018 | <a href="https://github.com/pfrcks">Github</a> | <a href="https://twitter.com/pfrcks">Twitter</a> | <a href="/index.xml">Subscribe</a>
  
  </footer>
  </body>
</html>

